{"cells":[{"cell_type":"markdown","source":["# Spark SQL and DataFrames"],"metadata":{}},{"cell_type":"markdown","source":["### 1. Basic manipulations\nFirst, let's create some users:"],"metadata":{}},{"cell_type":"code","source":["from datetime import date\n\n# Let's first create some users:\ncol_names = [\"first_name\", \"last_name\", \"birth_date\", \"gender\", \"country\"]\nusers = [\n  (\"Alice\", \"Jones\", date(1981, 4, 15), \"female\", \"Canada\"),\n  (\"John\", \"Doe\", date(1951, 1, 21), \"male\", \"USA\"),\n  (\"Barbara\", \"May\", date(1951, 9, 1), \"female\", \"Australia\"),\n  (\"James\", \"Smith\", date(1975, 7, 12), \"male\", \"United Kingdom\"),\n  (\"Gerrard\", \"Dupont\", date(1968, 5, 9), \"male\", \"France\"),\n  (\"Amanda\", \"B.\", date(1988, 12, 16), \"female\", \"New Zeland\")\n]\n\nusers_df = spark.createDataFrame(users, col_names)\ndisplay(users_df) # Only works in Databricks. Elswehere, use \"df.show()\" or \"df.toPandas()\""],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["Now it's your turn, create more users (at least 3, with different names) and add to the initial users, saving the result in a new variable."],"metadata":{}},{"cell_type":"code","source":["new_users_df = spark.createDataFrame([(\"Raphael\", \"Avoustin\", date(1995, 8, 19), \"male\", \"France\"),\n                                      (\"Tanguy\", \"Carroussel\", date(1995, 9, 11), \"male\", \"France\"),\n                                      (\"Mehdi\", \"Berraho\", date(1994, 7, 15), \"male\", \"France\")])\nall_users_df = users_df.union(new_users_df)\ndisplay(all_users_df) # or all_users_df.show()"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":["Now, select only two columns and show the resulting DataFrame, without saving it into a variable."],"metadata":{}},{"cell_type":"code","source":["all_users_df.select(\"last_name\",\"gender\").show()"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":["Now, register your DataFrame as a table and select the same two columns with a SQL query string"],"metadata":{}},{"cell_type":"code","source":["query_string = \"SELECT last_name, gender FROM new_view\"\nall_users_df.createOrReplaceTempView(\"new_view\") # creates a local temporary table accessible by a SQL query\nspark.sql(query_string).show()"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":["Now we want to add an unique identifier for each user in the table. There are many strategies for that, and for our example we will use the string `{last_name}_{first_name}`\n\nYou can use `all_users_df` since your latest operation did not override its values. Add a new column called `user_id` to your DataFrame and save to a new variable.\n\n**Hint:** The first place to look is in the [functions](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.functions) package"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import functions as fn\n\nusers_with_id=all_users_df.withColumn(\"user_id\",fn.concat_ws(\"_\", all_users_df.last_name, all_users_df.first_name))\ndisplay(users_with_id)"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":["You can also do the same thing with an User Defined Function by passing a lambda, although it is not recommended when there is already a function in the `functions` package.\n\nAdd a new column called `user_id_udf` to the DataFrame, using an UDF that receives two parameters and concatenate them."],"metadata":{}},{"cell_type":"code","source":["concat_udf = fn.udf(lambda x,y:x+'_'+y)\nusers_with_id_udf = all_users_df.withColumn('user_id',concat_udf(all_users_df.last_name, all_users_df.first_name))\ndisplay(users_with_id_udf)"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["Now, let's add another column called `age` with the computed age (in years) of each user, based on a given reference date, and save the resulting DataFrame into a new variable.\n\n**Hint:** You can first compute the age in months, and then divide by 12. A final operation will probably be needed to get an integer number."],"metadata":{}},{"cell_type":"code","source":["from dateutil.relativedelta import relativedelta\n\nreference_date = date(2017, 12, 31)\nreference_date_col=fn.lit(reference_date)\ndiff_date_udf = fn.udf(lambda x,y:relativedelta(x,y).__dict__['years'])\n\nusers_with_age = all_users_df.withColumn('age',diff_date_udf(reference_date_col,all_users_df.birth_date))\ndisplay(users_with_age)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["Now, an analytical question: How many users of each gender who are more than 40 years old exist in this data? The solution must be a DataFrame with two columns: `age` and `count` and two lines, one for each gender.\n\nBonus: Try to do your solution in a single chain (without intermediate variables)\n\n**Hint:** You will need to filter and aggregate the data."],"metadata":{}},{"cell_type":"code","source":["result = users_with_age.where(users_with_age.age > 40).groupBy('gender').count()\ndisplay(result)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### 2. Reading files, performing joins, and aggregating\n\nFor this section you will use some fake data of two datasets: `Users` and `Donations`. The data is provided in two CSV files *with header* and using *comma* as separator.\n\nThe `Users` dataset contains information about about the users. \n\nThe `Donations` dataset contains information about Donations performed by those users.\n\nThe first task is to read these files into the appropriate DataFrames.\n\n**Note:** You need to set the option \"inferSchema\" to true in order to have the columns in the correct types.\n\n_The data for this section has been created using [Mockaroo](https://www.mockaroo.com/)_."],"metadata":{}},{"cell_type":"code","source":["users_from_file = spark.read.csv('/FileStore/tables/users.csv', header=True,inferSchema=True)\ndonations = spark.read.csv('/FileStore/tables/donations.csv', header=True,inferSchema=True)"],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Now investigate the columns, contents and size of both datasets"],"metadata":{}},{"cell_type":"code","source":["# print the column names and types\nusers_from_file.printSchema()\ndonations.printSchema()\n\n# print 5 elements of the datasets in a tabular format\nusers_from_file.show(n=5)\ndonations.show(n=5)\n\n# print the number of lines of each dataset\nprint users_from_file.count()\nprint donations.count()"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["**Note:** If all the column types shown in the previous results are \"string\", you need to make sure you passed \"inferSchema\" as true when reading the CSV files before continuing.\n\nBefore using the data, we may want to add some information about the users. \n\nAdd a column containing the age of each user."],"metadata":{}},{"cell_type":"code","source":["reference_date = date(2017, 12, 31)\nreference_date_col=fn.lit(reference_date)\ndiff_date_udf = fn.udf(lambda x,y:relativedelta(x,y).__dict__['years'])\n\nusers_from_file = users_from_file.withColumn('age',diff_date_udf(reference_date_col,users_from_file.birth_date))\nusers_from_file.show()"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["Another useful information to have is the age **range** of each user. Using the `when` function, create the following 5 age ranges:\n- \"(0, 25]\"\n- \"(25, 35]\"\n- \"(35, 45]\"\n- \"(45, 55]\"\n- \"(55, ~]\"\n\nAnd add a new column to the users DataFrame, containing this information.\n\n**Note:** When building logical operations with Spark DataFrames, it's better to be add parantheses. Example:\n```python\ndf.select(\"name\", \"age\").where( (df(\"age\") > 20) & (df(\"age\") <= 30) )\n```\n\n**Note 2:** If you are having problems with the `when` function, you can make an User Defined Function and do your logic in standard python."],"metadata":{}},{"cell_type":"code","source":["users_from_file = users_from_file.withColumn(\"range\", fn.when((users_from_file.age >= 0) & (users_from_file.age <= 25),\"(0, 25]\")\\\n                                                     .when((users_from_file.age > 25) & (users_from_file.age <= 35),\"(25, 35]\")\\\n                                                     .when((users_from_file.age > 35) & (users_from_file.age <= 45),\"(35, 45]\")\\\n                                                     .when((users_from_file.age > 45) & (users_from_file.age <= 55),\"(45, 55]\")\\\n                                                     .when(users_from_file.age > 55,\"(55, ~]\"))\nusers_from_file.show()"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Now that we have improved our users' DataFrame, the first analysis we want to make is the average donation performed by each gender. However, the gender information and the donation value are in different tables, so first we need to join them, using the `user_id` as joining key.\n\n**Note:** Make sure you are not using the `users` DataFrame from the first part of the lab.\n\n**Note 2:** For better performance, you can [broadcast](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.broadcast) the smaller dataset. But you should only do this when you are sure the DataFrame fits in the memory of the nodes."],"metadata":{}},{"cell_type":"code","source":["joined_df = users_from_file.join(donations,'user_id')\ndisplay(joined_df)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Now, use aggregation to find the the min, max and avg donation by gender."],"metadata":{}},{"cell_type":"code","source":["donations_by_gender = joined_df.groupBy('gender').agg(fn.avg('donation_value'),fn.min('donation_value'),fn.max('donation_value'))\ndonations_by_gender.show()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Now, make the necessary transformations and aggregations to answer to the following questions about the data. Note that some questions are only about the users, so make sure to use the smaller possible dataset when looking for your answers!\n\n**Question 1:** \n\na) What's the average, min and max age of the users? \n\nb) What's the average, min and max age of the users, by gender?"],"metadata":{}},{"cell_type":"code","source":["result_1a = users_from_file.agg(fn.avg('age'),fn.min('age'),fn.max('age'))\nresult_1a.show()\n\nresult_1b = users_from_file.groupBy('gender').agg(fn.avg('age'),fn.min('age'),fn.max('age'))\nresult_1b.show()"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["**Question 2:**\n\na) How many distinct country origins exist in the data? Print a DataFrame listing them.\n\nb) What are the top 5 countries with the most users? Print a DataFrame containing the name of the countries and the counts."],"metadata":{}},{"cell_type":"code","source":["result_2a = users_from_file.groupBy('country').agg(fn.countDistinct('country'))\nresult_2a.show()\n\nresult_2b = users_from_file.groupBy('country').agg(fn.countDistinct('user_id')).sort(\"count(DISTINCT user_id)\",ascending = False)\nresult_2b.show(n=5)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["**Question 3:**\n\nWhat's the number of donations average, min and max donations values by age range?"],"metadata":{}},{"cell_type":"code","source":["result_3 = joined_df.groupBy('range').agg(fn.countDistinct('donation_value'),fn.avg('donation_value'),fn.max('donation_value'),fn.min('donation_value'))\n\nresult_3.show()"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["**Question 4:**\n\na) What's the number of donations, average, min and max donation values by user location (country)?\n\nb) What is the number of donations, average, min and max donation values by gender for each user location (contry)? (the resulting DataFrame must contain 5 columns: the gender of the user, their country and the 3 metrics)"],"metadata":{}},{"cell_type":"code","source":["result_4a = joined_df.groupBy('country').agg(fn.countDistinct('donation_value'),fn.avg('donation_value'),fn.max('donation_value'),fn.min('donation_value'))\nresult_4a.show()\n\nresult_4b = joined_df.groupBy('gender','country').agg(fn.countDistinct('donation_value'),fn.avg('donation_value'),fn.max('donation_value'),fn.min('donation_value'))\nresult_4b.show()"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"markdown","source":["**Question 5**\n\nWhich month of the year has the largest aggregated donation value?\n\n**Hint**: you can use a function to extract the month from a date, then you can aggregate to find the total donation value."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import year,month\nres=joined_df.withColumn(\"month\",month(joined_df.date))\nresult_5 =res.groupBy('month').agg(fn.sum('donation_value')).orderBy('sum(donation_value)',ascending=False)\nresult_5.show()\nresult_5.show(1)"],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"markdown","source":["### 3. Window Functions\n\n_This section uses the same data as the last one._\n\nWindow functions are very useful for gathering aggregated data without actually aggregating the DataFrame. They can also be used to find \"previous\" and \"next\" information for entities, such as an user. [This article](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html) has a very nice explanation about the concept."],"metadata":{}},{"cell_type":"markdown","source":["We want to find the users who donated less than a threshold and remove them from the donations dataset. Now, there are two ways of doing that:\n\n1) Performing a traditional aggregation to find the users who donated less than the threshold, then filtering these users from the donations dataset, either with `where(not(df(\"user_id\").isin(a_local_list)))` or with a join of type \"anti-join\".\n\n2) Using window functions to add a new column with the aggregated donations per user, and then using a normal filter such as `where(aggregated_donation < threshold)`\n\nLet's implement both and compare the complexity:\n\nFirst, perform the traditional aggregation and find the users who donated less than 500 in total:"],"metadata":{}},{"cell_type":"code","source":["res = joined_df.groupBy('user_id').agg(fn.sum('donation_value').alias('sum_donation_value'))\nbad_users=res.where(res.sum_donation_value<500)\nbad_users.show()"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["You should have found around 10 users. Now, perform an \"anti-join\" to remove those users from `joined_df`.\n\n**Hint:** The `join` operation accepts a third argument which is the join type. The accepted values are: 'inner', 'outer', 'full', 'fullouter', 'leftouter', 'left', 'rightouter', 'right', 'leftsemi', 'leftanti', 'cross'."],"metadata":{}},{"cell_type":"code","source":["good_donations = joined_df.join(bad_users, 'user_id', 'leftanti')\ngood_donations.count()"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["Verify if the count of `good_donations` makes sense by performing a normal join to find the `bad_donations`."],"metadata":{}},{"cell_type":"code","source":["bad_donations = joined_df.join(bad_users, 'user_id')\nbad_donations.count()"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["If you done everything right, at this point `good_donations.count()` + `bad_donations.count()` = `joined_df.count()`.\n\nBut using the join approach can be very heavy and it requires multipe operations. For this kind of problems, Window Functions are better."],"metadata":{}},{"cell_type":"code","source":["good_donations.count() + bad_donations.count() == joined_df.count()"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["The first step is to create your window specification over `user_id` by using partitionBy"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql import Window\n\nwindow_spec = Window.partitionBy('user_id')"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["Then, you can use one of the window functions of the `pyspark.sql.functions` package, appling to the created window_spec by using the `over` method. \n\n**Hint:** If you are blocked, try looking at the [documentation](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.over) for the `over` method, or searching on StackOverflow."],"metadata":{}},{"cell_type":"code","source":["new_column = fn.sum('donation_value').over(window_spec)\n\ndonations_with_total = joined_df.withColumn('total_donated_by_user', new_column)\ndonations_with_total.show()"],"metadata":{},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["And now you can just filter on the `total_donated_by_user` column:"],"metadata":{}},{"cell_type":"code","source":["good_donations_wf = donations_with_total.where(donations_with_total.total_donated_by_user>500)\ngood_donations_wf.count()"],"metadata":{},"outputs":[],"execution_count":54},{"cell_type":"markdown","source":["If you done everything right, you should obtain `good_donations_wf.count()` = `good_donations.count()`"],"metadata":{}},{"cell_type":"code","source":["good_donations_wf.count() == good_donations.count()"],"metadata":{},"outputs":[],"execution_count":56},{"cell_type":"markdown","source":["Window functions also can be useful to find the \"next\" and \"previous\" operations by using `functions.lead` and `functions.lag`. In our example, we can use it to find the date interval between two donations by a specific user.\n\nFor this kind of Window function, the window specification must be ordered by the date. So, create a new window specification partitioned by the `user_id` and ordered by the donation timestamp."],"metadata":{}},{"cell_type":"code","source":["ordered_window = Window.partitionBy('user_id').orderBy('timestamp')"],"metadata":{},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["Now use `functions.lag().over()` to add a column with the timestamp of the previous donation of the same user. Then, inspect the result to see what it looks like."],"metadata":{}},{"cell_type":"code","source":["new_column = fn.lag('timestamp').over(ordered_window)\ndonations_with_lag = good_donations.withColumn(\"last_donation\",new_column)\ndisplay(donations_with_lag.orderBy(\"user_id\", \"timestamp\"))"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["Finally, compute the average time it took for each user between two of their consecutive donations (in days), and print the 5 users with the smallest averages. The result must include at least the users' id, last name and birth date, as well as the computed average."],"metadata":{}},{"cell_type":"code","source":["diff_date_udf = fn.udf(lambda x,y:(x-y).days)\n\ndonations_with_lag_notnull=donations_with_lag.where(donations_with_lag.last_donation.isNotNull())\n\ndonations_with_lag_notnull=donations_with_lag_notnull.withColumn(\"diff\",diff_date_udf(donations_with_lag_notnull.timestamp,donations_with_lag_notnull.last_donation))\n\nusers_average_between_donations=donations_with_lag_notnull.groupBy(\"user_id\").agg(fn.avg(\"diff\").alias('avg_time_by_user')).orderBy('avg_time_by_user',ascending=True)\nusers_average_between_donations.show(5)\n"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["Congratulations, you have finished this notebook!"],"metadata":{}}],"metadata":{"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2},"version":"2.7.9","nbconvert_exporter":"python","file_extension":".py"},"name":"3-spark-sql","notebookId":4171995517130857},"nbformat":4,"nbformat_minor":0}
